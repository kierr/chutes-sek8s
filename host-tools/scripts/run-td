#!/usr/bin/env python3

# Canonical TDX run script — minimally extended for:
#   - config volume
#   - cache volume
#   - ssh port override
#   - TAP networking

import argparse
import os
import platform
import re
import signal
import subprocess
import time
import sys

file_path = os.path.realpath(os.path.dirname(__file__))

pidfile = '/tmp/tdx-td-pid.pid'
logfile = '/tmp/tdx-guest-td.log'
process_name = 'chutes-td'

ubuntu_version = platform.freedesktop_os_release().get('VERSION_ID')

# TDVF MUST NOT be overridden (MRTD depends on it)
firmware = '../../firmware/TDVF.fd'


# Canonical resource sizing for the production guest
DEFAULT_MEM = '100G'
DEFAULT_VCPUS = '32'

# GPU BAR size mappings (in MB)
# B200 requires 512GB (524288 MB) recommended, 384GB (393216 MB) minimum
# H200 uses 256GB (262144 MB)
GPU_BAR_SIZES = {
    'B200': 524288,  # 512GB recommended
    'H200': 262144,  # 256GB
    'default': 262144,  # Default to H200 size for unknown GPUs
}


def do_print(ssh_port):
    try:
        with open(pidfile) as pid_file:
            pid = int(pid_file.read())
            print(f'TDX VM running with PID: {pid}')
            print(f'Login:')
            print(f'   ssh -p {ssh_port} tdx@localhost   (default: tdx/123456)')
            print(f'   ssh -p {ssh_port} root@localhost  (password: 123456)')
    except:
        pass


def do_clean():
    print('Clean VM')
    try:
        with open(pidfile) as pid_file:
            pid = int(pid_file.read())
            os.kill(pid, signal.SIGTERM)
            time.sleep(3)
        os.remove(pidfile)
    except FileNotFoundError:
        pass


def add_vsock(cmd):
    cmd.extend(['-device', 'vhost-vsock-pci,guest-cid=3'])


# NVIDIA PCI vendor ID (matches setup-gpus.sh NVIDIA_VENDOR_ID)
_NVIDIA_VENDOR = '10de'


def _lspci_nvidia_lines() -> list[str]:
    """lspci -Dnn lines for NVIDIA devices (vendor 10de).

    We intentionally use -D so BDFs are always in full domain form (0000:bb:dd.f),
    matching nvidia-gpu-tools output and sysfs/virsh expectations.
    """
    output = subprocess.check_output(["lspci", "-Dnn"], stderr=subprocess.STDOUT)
    return [line for line in output.decode().splitlines() if _NVIDIA_VENDOR in line]

def detect_nvidia_gpus() -> list[str]:
    """Detect NVIDIA GPU BDFs via lspci: vendor 10de, description contains H200 or B200 (matches setup-gpus.sh intent)."""
    devices = []
    for line in _lspci_nvidia_lines():
        parts = line.strip().split()
        if not parts:
            continue
        bdf = parts[0]
        # Match setup-gpus.sh: grep for device name in description (H200, B200)
        if 'B200' in line or 'H200' in line:
            devices.append(bdf)
    return sorted(devices)


def get_gpu_bdfs() -> list[str] | None:
    """Get GPU BDFs from nvidia_gpu_tools --query-cc-mode (matches setup-gpus.sh gpus_bdfs()).

    Used for both H200 and B200 so device selection is consistent and tool-driven.
    Returns None if the tool is unavailable or returns no GPUs.
    """
    try:
        cmd = ensure_gpu_tools_available()
        out = subprocess.run(
            [cmd, '--query-cc-mode'],
            capture_output=True,
            text=True,
            timeout=30,
        )
        if out.returncode != 0:
            return None
        # Match setup-gpus.sh: "  N GPU BDF" -> BDF (regex uses [0-9] for domain, not [0-0])
        bdf_re = re.compile(
            r'\s+\d+\s+GPU\s+([0-9a-f]{4}:[0-9a-f]{2,4}:[0-9a-f]{2}\.[0-9])',
            re.IGNORECASE,
        )
        bdfs = []
        for line in (out.stdout or '').splitlines():
            m = bdf_re.search(line)
            if m:
                bdfs.append(m.group(1))
        return sorted(bdfs) if bdfs else None
    except (FileNotFoundError, subprocess.TimeoutExpired, ValueError, RuntimeError):
        return None


def detect_nvswitches() -> list[str]:
    """Detect NVSwitch BDFs via lspci: vendor 10de, description contains NVSwitch (matches setup-gpus.sh)."""
    devices = []
    for line in _lspci_nvidia_lines():
        parts = line.strip().split()
        if not parts:
            continue
        if 'NVSwitch' in line:
            devices.append(parts[0])
    return sorted(devices)


def get_gpu_models_from_lspci(bdfs: list[str]) -> dict[str, str]:
    """Get GPU model per BDF from lspci: vendor 10de, grep for B200 or H200 in description (matches setup-gpus.sh).

    Returns dict BDF -> 'B200' | 'H200' | 'default'. Check B200 before H200 (substring order).
    """
    bdf_set = set(bdfs)
    result = {}
    for line in _lspci_nvidia_lines():
        parts = line.strip().split()
        if not parts:
            continue
        bdf = parts[0]
        if bdf not in bdf_set:
            continue
        # Match setup-gpus.sh: device name in lspci description (B200 before H200)
        if 'B200' in line:
            result[bdf] = 'B200'
        elif 'H200' in line:
            result[bdf] = 'H200'
        else:
            result[bdf] = 'default'
    return result


def get_gpu_bar_size(gpu_addr: str, gpu_models: dict[str, str]) -> int:
    """Get the BAR size (in MB) for a specific GPU from lspci-derived model (B200, H200)."""
    model = gpu_models.get(gpu_addr, 'default')
    if model not in ['B200', 'H200']:
        raise ValueError(
            f"Unsupported GPU model for {gpu_addr} (lspci: {model}). "
            "Only B200 and H200 GPUs are supported."
        )
    bar_size = GPU_BAR_SIZES.get(model, GPU_BAR_SIZES['default'])
    print(f'    GPU {gpu_addr}: {model} detected, using {bar_size} MB BAR')
    return bar_size


def ensure_gpu_tools_available() -> str:
    """Ensure nvidia-gpu-tools CLI is available.
    
    Checks if nvidia-gpu-tools is in PATH. If not, installs from bundled wheel into a venv
    and creates a system-wide symlink. This handles externally-managed Python environments.
    
    Returns:
        Command string to use for nvidia-gpu-tools (either 'nvidia-gpu-tools' or path)
    
    Raises:
        FileNotFoundError: If bundled wheel file is not found
        RuntimeError: If python3 is not available or installation fails
        subprocess.CalledProcessError: If installation fails
    """
    # Check if already in PATH
    result = subprocess.run(['which', 'nvidia-gpu-tools'], 
                          capture_output=True)
    if result.returncode == 0:
        return 'nvidia-gpu-tools'
    
    # Check if python3 is available
    result = subprocess.run(['which', 'python3'], capture_output=True)
    if result.returncode != 0:
        raise RuntimeError(
            "python3 is not available. Please install python3 to install GPU admin tools."
        )
    
    # Check if venv module is available
    result = subprocess.run(['python3', '-m', 'venv', '--help'], 
                          capture_output=True)
    if result.returncode != 0:
        # Try to get python version for more specific error message
        version_result = subprocess.run(['python3', '--version'], 
                                      capture_output=True)
        python_version = version_result.stdout.decode().strip() if version_result.returncode == 0 else "python3"
        raise RuntimeError(
            f"The python3-venv package is not installed. "
            f"Please install it using: sudo apt install python3-venv\n"
            f"For Python 3.13 specifically: sudo apt install python3.13-venv\n"
            f"After installing, the script will automatically create a virtual environment "
            f"and install the GPU admin tools."
        )
    
    # Install from bundled wheel into a venv
    bundled_tools_dir = f'{file_path}/gpu-tools'
    if not os.path.exists(bundled_tools_dir):
        raise FileNotFoundError(
            f"GPU tools directory not found: {bundled_tools_dir}. "
            "Expected a .whl file to be committed to the repository."
        )
    
    wheel_files = [f for f in os.listdir(bundled_tools_dir) if f.endswith('.whl')]
    
    if not wheel_files:
        raise FileNotFoundError(
            f"No bundled GPU tools wheel found in {bundled_tools_dir}. "
            "Expected a .whl file to be committed to the repository."
        )
    
    wheel_file = os.path.join(bundled_tools_dir, wheel_files[0])
    venv_dir = os.path.join(bundled_tools_dir, 'venv')
    venv_python = os.path.join(venv_dir, 'bin', 'python')
    venv_pip = os.path.join(venv_dir, 'bin', 'pip')
    venv_bin = os.path.join(venv_dir, 'bin')
    cli_symlink = '/usr/local/bin/nvidia-gpu-tools'
    
    # Create venv if it doesn't exist
    if not os.path.exists(venv_dir):
        print(f'  Creating virtual environment for GPU admin tools...')
        try:
            subprocess.check_call(['sudo', 'python3', '-m', 'venv', venv_dir], 
                               stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
            # Provide helpful error message if venv creation fails
            raise RuntimeError(
                f"Failed to create virtual environment: {e}\n"
                f"The python3-venv package may not be installed. "
                f"Please install it using: sudo apt install python3-venv\n"
                f"For Python 3.13 specifically: sudo apt install python3.13-venv"
            )
    
    # Ensure pip is available in the venv
    if not os.path.exists(venv_pip):
        print(f'  Bootstrapping pip in virtual environment...')
        try:
            # Try to bootstrap pip using ensurepip
            subprocess.check_call(['sudo', venv_python, '-m', 'ensurepip', '--upgrade'], 
                               stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError:
            # If ensurepip fails, try to install pip using get-pip.py
            # This is a fallback if ensurepip is not available
            raise RuntimeError(
                "pip is not available in the virtual environment and could not be bootstrapped. "
                "The python3-venv package may need to be reinstalled, or you may need to install "
                "python3-pip separately: sudo apt install python3-pip"
            )
    
    # Install wheel into venv
    print(f'  Installing GPU admin tools from bundled wheel: {os.path.basename(wheel_file)}')
    subprocess.check_call(['sudo', venv_pip, 'install', '--quiet', '--upgrade', wheel_file])
    
    # Find the CLI entry point in the venv
    cli_in_venv = os.path.join(venv_bin, 'nvidia-gpu-tools')
    
    # Verify the entry point exists and works
    if not os.path.exists(cli_in_venv):
        raise RuntimeError(
            "nvidia-gpu-tools CLI not found in venv after installation. "
            "The wheel may not have installed correctly or the entry point is misconfigured."
        )
    
    # Test if the entry point works
    test_result = subprocess.run([cli_in_venv, '--help'], 
                                capture_output=True, timeout=5)
    if test_result.returncode != 0:
        error_msg = test_result.stderr.decode() if test_result.stderr else "Unknown error"
        raise RuntimeError(
            f"nvidia-gpu-tools CLI entry point is broken. "
            f"The wheel was not built correctly. Error: {error_msg}\n"
            f"Please rebuild the wheel using: cd {bundled_tools_dir} && ./bundle-tools.sh"
        )
    
    # Create system-wide symlink
    if os.path.exists(cli_symlink):
        # Remove existing symlink if it exists
        if os.path.islink(cli_symlink):
            subprocess.check_call(['sudo', 'rm', cli_symlink])
        else:
            # If it's not a symlink, we should warn but not overwrite
            raise RuntimeError(
                f"Cannot create symlink: {cli_symlink} exists and is not a symlink. "
                "Please remove it manually and try again."
            )
    
    # Create the symlink (always create it, even if we just removed an existing one)
    print(f'  Creating system-wide symlink: {cli_symlink}')
    subprocess.check_call(['sudo', 'ln', '-s', cli_in_venv, cli_symlink])
    
    # Verify it's now available
    result = subprocess.run(['which', 'nvidia-gpu-tools'],
                          capture_output=True)
    if result.returncode == 0:
        return 'nvidia-gpu-tools'
    else:
        raise RuntimeError(
            "nvidia-gpu-tools installation succeeded but CLI not found in PATH. "
            f"Symlink created at {cli_symlink}, but it may not be in your PATH."
        )


def get_gpu_tool_args(gpu_addr: str, model: str, total_gpus: int) -> list[list[str]]:
    """Get nvidia-gpu-tools arguments for a GPU (model from lspci: B200, H200).

    Returns a list of command argument lists (each sublist is one command to run).
    """
    if model == 'B200':
        # B200 always uses CC mode (no NVSwitches); no PPCIe
        return [['--set-cc-mode=on', '--reset-after-cc-mode-switch']]
    if model == 'H200':
        # H200: PPCIe when 8 GPUs (matches setup-gpus.sh)
        if total_gpus == 8:
            return [
                ['--set-cc-mode=off', '--reset-after-cc-mode-switch'],
                ['--set-ppcie-mode=on', '--reset-after-ppcie-mode-switch']
            ]
        return [
            ['--set-ppcie-mode=off', '--reset-after-ppcie-mode-switch'],
            ['--set-cc-mode=on', '--reset-after-cc-mode-switch']
        ]
    raise ValueError(
        f"Unsupported GPU model for {gpu_addr} (lspci: {model}). "
        "Only B200 and H200 GPUs are supported."
    )


def load_vfio_modules():
    """Load VFIO kernel modules required for PCI passthrough."""
    modules = ['vfio_pci', 'vfio_iommu_type1', 'vfio_virqfd']
    for module in modules:
        try:
            subprocess.run(['modprobe', module], check=False, 
                         capture_output=True)
        except Exception:
            pass  # Module may already be loaded


def bind_device_to_vfio(device_bdf: str):
    """Bind a single device to vfio-pci using driver_override method."""
    driver_override_path = f'/sys/bus/pci/devices/{device_bdf}/driver_override'
    try:
        with open(driver_override_path, 'w') as f:
            f.write('vfio-pci')
        # Trigger driver probe
        with open('/sys/bus/pci/drivers_probe', 'w') as f:
            f.write(device_bdf)
    except Exception as e:
        print(f'  Warning: Failed to bind {device_bdf} to vfio-pci: {e}')


def bind_explicit_devices_to_vfio(devices: list[str]):
    """Bind only the given BDFs to vfio-pci (no IOMMU group binding).

    Matches setup-gpus.sh semantics: explicit device list only, no bridges
    or unrelated fabric endpoints. Deterministic and reproducible across boots.
    """
    load_vfio_modules()
    for device in devices:
        bind_device_to_vfio(device)
        print(f'    {device} → vfio-pci')


def prepare_gpus(gpus, nvswitches, gpu_models: dict[str, str]):
    """Prepare GPUs and NVSwitches for passthrough.

    Order must match tdx/gpu-cc/h100/setup-gpus.sh: configure mode (CC/PPCIe)
    with nvidia_gpu_tools before binding to vfio-pci. There are no nvidia
    drivers on the host; binding to vfio-pci first would prevent the tools
    from configuring the devices and can cause NVSwitches to be missing in
    the VM.

    Args:
        gpus: List of GPU BDF addresses
        nvswitches: List of NVSwitch BDF addresses
        gpu_models: Dict mapping GPU BDF to model string (B200, H200) from lspci
    """
    # Step 1: Ensure GPU tools are available
    print('  Ensuring GPU admin tools are available...')
    nvidia_tools_cmd = ensure_gpu_tools_available()
    cmd_base = ['sudo', nvidia_tools_cmd]

    total_gpus = len(gpus)
    # H200 PPCIe when 8 GPUs (matches setup-gpus.sh); B200 never uses PPCIe
    has_h200 = any(gpu_models.get(g, '') == 'H200' for g in gpus)
    is_ppcie_mode = has_h200 and total_gpus == 8

    # Step 2: Configure NVSwitches FIRST before binding to vfio (matches setup-gpus.sh)
    if is_ppcie_mode and nvswitches:
        print('  Configuring NVSwitches for PPCIe mode...')
        for nvsw in nvswitches:
            print(f'  Preparing NVSwitch {nvsw} for PPCIe')
            subprocess.check_call(
                cmd_base + ['--set-cc-mode=off', '--reset-after-cc-mode-switch', f'--gpu-bdf={nvsw}'],
                stderr=subprocess.STDOUT
            )
            subprocess.check_call(
                cmd_base + ['--set-ppcie-mode=on', '--reset-after-ppcie-mode-switch', f'--gpu-bdf={nvsw}'],
                stderr=subprocess.STDOUT
            )

    # Step 3: Configure each GPU before binding to vfio (matches setup-gpus.sh)
    print('  Configuring GPUs...')
    for gpu in gpus:
        model = gpu_models.get(gpu, 'default')
        if model not in ('B200', 'H200'):
            raise ValueError(
                f"Unsupported GPU model for {gpu} (lspci: {model}). "
                "Only B200 and H200 GPUs are supported."
            )

        tool_args_list = get_gpu_tool_args(gpu, model, total_gpus)

        if model == 'B200':
            mode_str = 'CC mode (B200)'
        elif is_ppcie_mode:
            mode_str = 'PPCIe mode'
        else:
            mode_str = 'CC mode'

        print(f'  Preparing GPU {gpu} ({model}) for {mode_str}')

        for tool_args in tool_args_list:
            subprocess.check_call(
                cmd_base + tool_args + [f'--gpu-bdf={gpu}'],
                stderr=subprocess.STDOUT
            )

    # Step 4: Bind explicit BDF list to vfio-pci (no IOMMU group binding; matches setup-gpus.sh)
    # B200: GPUs only. H200: GPUs + NVSwitches when PPCIe (8 GPUs).
    has_b200 = any(gpu_models.get(g, '') == 'B200' for g in gpus)
    if has_b200:
        devices_to_bind = gpus
    else:
        devices_to_bind = gpus + nvswitches if is_ppcie_mode and nvswitches else gpus
    print('  Binding devices to vfio-pci (explicit BDF list)...')
    bind_explicit_devices_to_vfio(devices_to_bind)

    # Step 5: Virsh binding for each GPU (matches setup-gpus.sh behavior)
    for gpu in gpus:
        virsh_gpu_bdf = gpu.replace(':', '_').replace('.', '_')
        print(f'  Binding GPU {gpu} to vfio-pci via virsh')
        subprocess.check_call(
            ['sudo', 'virsh', 'nodedev-reattach', f'pci_{virsh_gpu_bdf}'],
            stderr=subprocess.DEVNULL
        )
        subprocess.check_call(
            ['sudo', 'virsh', 'nodedev-detach', f'pci_{virsh_gpu_bdf}'],
            stderr=subprocess.STDOUT
        )

    # Step 6: Install udev rules only if they do not already exist (host-level state)
    udev_rules_src = f'{file_path}/vfio-passthrough.rules'
    udev_rules_dst = '/etc/udev/rules.d/vfio-passthrough.rules'
    if not os.path.exists(udev_rules_src):
        raise FileNotFoundError(
            f"Udev rules file not found: {udev_rules_src}. "
            "This file should be in the same directory as run-td."
        )
    if not os.path.exists(udev_rules_dst):
        print('  Installing udev rules...')
        subprocess.check_call(
            ['sudo', 'cp', udev_rules_src, '/etc/udev/rules.d/'],
            stderr=subprocess.STDOUT
        )
        subprocess.check_call(
            ['sudo', 'udevadm', 'control', '--reload-rules'],
            stderr=subprocess.STDOUT
        )
        subprocess.check_call(
            ['sudo', 'udevadm', 'trigger'],
            stderr=subprocess.STDOUT
        )
    else:
        print('  Udev rules already present (skipping install)')


def add_gpu_passthrough(cmd):
    """Discover + prepare GPUs (and NVSwitch when required) for passthrough.

    This mirrors setup-gpus.sh behavior:
    - GPUs are enumerated via nvidia-gpu-tools --query-cc-mode (fallback to lspci).
    - NVSwitch devices are only enumerated (via lspci) when topology requires them (H200 + 8 GPUs).
    """
    # Primary source of truth (setup-gpus.sh): nvidia_gpu_tools --query-cc-mode
    gpus = get_gpu_bdfs()
    if not gpus:
        # Fallback only (e.g. older tool versions / unexpected environments)
        gpus = detect_nvidia_gpus()

    if not gpus:
        return

    # Model from lspci: vendor 10de, grep B200 / H200 in description (matches setup-gpus.sh)
    gpu_models = get_gpu_models_from_lspci(gpus)
    has_h200 = any(gpu_models.get(g, '') == 'H200' for g in gpus)
    has_b200 = any(gpu_models.get(g, '') == 'B200' for g in gpus)

    # setup-gpus.sh only considers NVSwitch for the 8-GPU PPCIe (H200) topology.
    # B200 treats NVSwitch as integrated CC fabric: do not enumerate or pass through.
    nvswitches = detect_nvswitches() if (has_h200 and len(gpus) == 8 and not has_b200) else []

    print(f'  Detected {len(gpus)} GPUs: {gpus}')
    print(f'  Detected {len(nvswitches)} NVSwitches: {nvswitches}')

    # Mode: B200 = CC only; H200 = PPCIe when 8 GPUs (matches setup-gpus.sh, no NVSwitch count check)
    if has_b200:
        print('  CC mode: enabled (B200 GPUs detected - B200 architecture uses CC mode, no NVSwitches)')
        if nvswitches:
            print(f'  Warning: B200 GPUs detected but {len(nvswitches)} NVSwitch(es) found (B200 does not use NVSwitches)')
    elif len(gpus) == 8 and has_h200:
        print('  PPCIe mode: enabled (8 GPUs, H200)')
    elif nvswitches:
        print(f'  Standard passthrough mode with {len(nvswitches)} NVSwitch(es)')
    else:
        print('  CC mode: enabled (standard configuration)')

    prepare_gpus(gpus, nvswitches, gpu_models)
    cmd.extend(['-object', 'iommufd,id=iommufd0'])

    # B200: no NVSwitch in VM (integrated CC fabric). H200: pass NVSwitches when PPCIe.
    nvswitches_for_vm = [] if has_b200 else nvswitches

    port = 16
    slot = 0x5
    func = 0

    print(f'  Adding {len(gpus)} GPU(s) to PCI topology...')
    for i, gpu in enumerate(gpus):
        rp_id = f'rp{i+1}'
        chassis = i+1

        if func == 0:
            cmd.extend([
                '-device',
                f'pcie-root-port,port={port},chassis={chassis},id={rp_id},'
                f'bus=pcie.0,multifunction=on,addr={slot:#x}'
            ])
        else:
            cmd.extend([
                '-device',
                f'pcie-root-port,port={port},chassis={chassis},id={rp_id},'
                f'bus=pcie.0,addr={slot:#x}.{func:#x}'
            ])

        # Get BAR size for this specific GPU
        bar_size_mb = get_gpu_bar_size(gpu, gpu_models)

        cmd.extend([
            '-device', f'vfio-pci,host={gpu},bus={rp_id},addr=0x0,iommufd=iommufd0',
            '-fw_cfg', f'name=opt/ovmf/X-PciMmio64Mb{i+1},string={bar_size_mb}'
        ])

        port += 1
        func = (func + 1) % 8
        if func == 0:
            slot += 1

    if nvswitches_for_vm:
        print(f'  Adding {len(nvswitches_for_vm)} NVSwitch(es) to PCI topology...')

    for j, nvsw in enumerate(nvswitches_for_vm):
        rp_id = f'rp_nvsw{j+1}'
        chassis = len(gpus) + j + 1

        if func == 0:
            addr = f'{slot:#x}.{func:#x}'
            multifunc = 'multifunction=on'
        else:
            addr = f'{slot:#x}.{func:#x}'
            multifunc = None

        cmd.extend([
            '-device',
            f'pcie-root-port,port={port},chassis={chassis},id={rp_id},'
            f'bus=pcie.0,{multifunc+","+("") if multifunc else ""}addr={addr}'
        ])

        cmd.extend([
            '-device', f'vfio-pci,host={nvsw},bus={rp_id},addr=0x0,iommufd=iommufd0'
        ])

        port += 1
        func = (func + 1) % 8
        if func == 0:
            slot += 1
    print(f'  GPU passthrough configured: {len(gpus)} GPU(s), {len(nvswitches_for_vm)} NVSwitch(es)')


def build_network(cmd, args):
    """Option A: TAP overrides Canonical networking completely."""
    if args.network_type == "tap":
        if not args.net_iface:
            print("ERROR: --network-type tap requires --net-iface")
            sys.exit(1)

        print(f"Networking: TAP mode (iface={args.net_iface})")
        cmd.extend([
            '-netdev', f'tap,id=n0,ifname={args.net_iface},script=no,downscript=no',
            '-device', 'virtio-net-pci,netdev=n0,mac=52:54:00:12:34:56',
        ])
        return

    print("Networking: Canonical user-mode networking")
    cmd.extend([
        '-device', 'virtio-net-pci,netdev=nic0_td',
        '-netdev', f'user,id=nic0_td,hostfwd=tcp::{args.ssh_port}-:22',
    ])


def do_run(img_path, pass_gpus, config_volume, cache_volume, storage_volume,
        ssh_port, foreground, args):

    mem = DEFAULT_MEM
    vcpus = DEFAULT_VCPUS

    print(f'Launching TDX VM: {vcpus} vCPUs, {mem} RAM')
    print(f'Image: {img_path}')

    cpu_args = 'host' if ubuntu_version == '24.04' else 'host,-avx10'

    qemu_cmds = [
        'qemu-system-x86_64',
        '-accel', 'kvm',
        '-m', mem,
        '-smp', vcpus,
        '-name', f'{process_name},process={process_name},debug-threads=on',
        '-cpu', cpu_args,
        '-object', '{"qom-type":"tdx-guest","id":"tdx","quote-generation-socket":{"type":"vsock","cid":"2","port":"4050"}}',
        '-object', f'memory-backend-ram,id=mem0,size={mem}',
        '-machine', 'q35,kernel_irqchip=split,confidential-guest-support=tdx,memory-backend=mem0',
        '-bios', firmware,
        '-nodefaults',
        '-vga', 'none',
    ]

    # Serial logic (canonical restore)
    if foreground:
        qemu_cmds.extend(['-nographic', '-serial', 'mon:stdio'])
    else:
        qemu_cmds.extend([
            '-nographic',
            '-serial', f'file:{logfile}',
            '-daemonize',
            '-pidfile', pidfile,
        ])

    # Boot disk
    qemu_cmds.extend([
        '-drive', f'file={img_path},if=none,id=virtio-disk0',
        '-device', 'virtio-blk-pci,drive=virtio-disk0',
    ])

    # Networking
    build_network(qemu_cmds, args)

    # Config + cache volumes
    if config_volume:
        qemu_cmds.extend([
            '-drive', f'file={config_volume},if=virtio,format=qcow2,readonly=on'
        ])
    if cache_volume:
        qemu_cmds.extend([
            '-drive', f'file={cache_volume},if=virtio,cache=none,format=qcow2'
        ])
    if storage_volume:
        qemu_cmds.extend([
            '-drive', f'file={storage_volume},if=virtio,cache=none,format=qcow2'
        ])

    add_vsock(qemu_cmds)

    if pass_gpus:
        add_gpu_passthrough(qemu_cmds)

    print('Launching QEMU...')
    subprocess.run(qemu_cmds, stderr=subprocess.STDOUT)

    if not foreground:
        print(f'Log file: {logfile}')
    do_print(ssh_port)


def run_td(args):
    try:
        do_clean()
    except:
        pass

    if args.clean:
        return

    if not args.image:
        print("Error: --image is required")
        sys.exit(1)

    img = args.image

    do_run(
        img_path=img,
        pass_gpus=args.pass_gpus,
        config_volume=args.config_volume,
        cache_volume=args.cache_volume,
        storage_volume=args.storage_volume,
        ssh_port=args.ssh_port,
        foreground=args.foreground,
        args=args,
    )


if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # Canonical args
    parser.add_argument("--image", type=str, help="Path to VM image")
    parser.add_argument("--pass-gpus", action='store_true')
    parser.add_argument("--foreground", action='store_true')
    parser.add_argument("--clean", action='store_true')

    # Chutes additions
    parser.add_argument("--config-volume", type=str)
    parser.add_argument("--cache-volume", type=str)
    parser.add_argument("--storage-volume", type=str, help="Storage volume for VM storage (containerd and kubelet-pods)")
    parser.add_argument("--ssh-port", type=int, default=10022)

    # Networking
    parser.add_argument("--network-type", choices=["tap", "user"], default="user")
    parser.add_argument("--net-iface", type=str)

    args = parser.parse_args()

    run_td(args)
