# End-to-End Chutes Miner Setup

This guide combines the host automation in `host-tools/`, the k3s-based TDX guest image built under `ansible/k3s/`, and the control-plane automation in the public [chutes-miner](https://github.com/chutesai/chutes-miner) repo into a single workflow. Use it when you want to take a baremetal TDX-capable machine all the way to a GPU-backed miner that registers with the Chutes network.

> **Who is this for?** Operators who already have TDX hardware and want clarity on how the host preparation, VM launch, and in-guest miner deployment fit together.

---

## üì¶ Components at a Glance

| Layer | What lives here | This repo path |
| --- | --- | --- |
| **Host** | TDX kernel settings, GPU binding, bridge networking, cache/config volume creation, VM orchestration | `host-tools/`, `tdx/`
| **Guest Image** | Encrypted Ubuntu root disk with k3s, attestation services, GPU plumbing, admission policies | Built via `ansible/k3s`
| **Config Volume** | Hostname, miner SS58, miner seed, static netplan config consumed on boot | Generated by `host-tools/scripts/create-config.sh`
| **k3s Cluster (inside VM)** | Ships with Chutes system workloads already installed‚Äîno Helm deploys happen on the TEE node | `/etc/rancher/k3s/k3s.yaml`
| **Control Node** | Runs the `chutes-miner` components to manage your inventory (TEE + non-TEE) | [chutes-miner repo](https://github.com/chutesai/chutes-miner)

---

## ‚úÖ Pre-flight Checklist

- Intel TDX-capable server (Ubuntu 25.04 host, NVIDIA H100/H200 GPUs, NVSwitch optional)
- Intel PCCS access + API key (for PCK cert registration)
- The `tdx-guest.qcow2` image downloaded from R2
- Miner credentials: SS58 address and secret seed without `0x`
- Control node provisioned with the [chutes-miner](https://github.com/chutesai/chutes-miner) Ansible roles
- `chutes-miner-cli` installed on that control node to manage miner inventory
- `kubectl` access to the TEE VM's bridged k3s endpoint for verification (Helm **not** required on the TEE node)

---

## üîê Attestation + LUKS Key Release

The shipped guest image boots into a **fully LUKS-encrypted root filesystem**. The decryption key is never stored on disk; it is released only when the VM presents a valid Intel TDX quote that matches the measurements recorded by the Chutes attestation service. In other words:

- You cannot bypass the boot gate by mounting the qcow2 elsewhere‚Äîthe root partition remains locked without the remote key.
- Building the VM image yourself is insufficient unless you also control the policy + key server that validates MRTD/RTMR and hands back the LUKS secret.
- Miner operators simply supply SS58/seed values on the config volume; **they never see or manage the root-disk passphrase**.

Keep this in mind when planning disaster recovery: you need access to the attestation/key service (or an equivalent under your control) to revive a VM, even if you possess the qcow2 artifact.

---

## üó∫Ô∏è Workflow Overview

1. **Prepare the host** ‚Äì enable TDX in firmware + kernel, install PCCS, configure GPUs. *(host-tools README)*
2. **Fetch the guest image** ‚Äì download `tdx-guest.qcow2` into `guest-tools/image/`.
3. **Create configuration** ‚Äì generate `config.yaml` to provide deployment configuration.
4. **Launch the VM** ‚Äì run `./quick-launch.sh` to bind GPUs, create cache/config volumes, build the network bridge, and start QEMU.
5. **Tie into the miner control plane** ‚Äì from your control node, add the new TEE VM to your miner inventory with `chutes-miner-cli`.
6. **Operate & monitor** ‚Äì follow the log, upgrade paths, and troubleshooting tips listed below.

Each step is detailed in the following sections.

---

## 1. Prepare the Host

Follow the dedicated [TDX VM Host Setup Guide](../host-tools/README.md). High-level tasks:

1. Clone this repository and initialize the `tdx/` submodule.
2. Edit `tdx/setup-tdx-config` (set `TDX_SETUP_ATTESTATION=1`) and run `tdx/setup-tdx-host.sh`.
3. Reboot into the TDX-enabled kernel and verify `dmesg | grep -i tdx`.
4. Configure PCCS (`pccs-configure`, restart the service, run `PCKIDRetrievalTool`).
5. Install Python + PyYAML (`pip3 install pyyaml`) for the orchestration scripts.

When the host guide says "proceed to launch," return here to continue.

---

## 2. Download the Guest Image

The orchestration defaults to `guest-tools/image/tdx-guest.qcow2`. Fetch it from the R2 bucket:

```bash
cd guest-tools/image
curl -O https://vm.chutes.ai/tdx-guest.qcow2
```

If you have multiple images (debug vs production), export `TD_IMG` before launching to override the default.

---

## 3. Create the Miner Configuration

Generate a template config and customize it with your network + credentials:

```bash
cd host-tools/scripts
./quick-launch.sh --template
nano config.yaml
```

Key fields:

```yaml
vm:
  hostname: chutes-miner-tee-0
  ip: 192.168.100.2
  bridge_cidr: 192.168.100.1/24
  public_interface: ens9f0np0
miner:
  ss58: "<your_ss58>"
  seed: "<your_seed_no_0x>"
volumes:
  cache:
    size: 500G
advanced:
  memory: 100G
  vcpus: 32
```

Behind the scenes `quick-launch.sh` also calls `create-config.sh`, which writes the same hostname and credentials into a qcow2 volume mounted at `/var/config` inside the VM. First-boot scripts pick those up and create the `chutes/miner-credentials` Kubernetes secret automatically.

> **Tip:** You can override any field without editing YAML by adding CLI flags such as `--miner-ss58`, `--mem 128G`, or `--network-type user` when you launch.

---

## 4. Launch the TDX VM

From `host-tools/scripts` run:

```bash
./quick-launch.sh config.yaml --foreground
```

What this does:

1. Validates the host (`kvm_intel.tdx=1`) and presence of required tools.
2. Binds NVIDIA GPUs + NVSwitches to `vfio-pci` unless `--skip-bind` is passed.
3. Ensures a cache volume exists (or skips it with `--skip-cache`).
4. Recreates the config volume with your current miner credentials.
5. Builds the NAT-backed bridge network (`br0` + TAP) tied to `public_interface`.
6. Invokes `run-td` to boot the QCOW2 with all devices attached.

Logs stream to `/tmp/tdx-guest-td.log` (or your terminal when `--foreground` is set). The VM PID is tracked in `/tmp/tdx-td-pid.pid` for later cleanup.

---

## 5. Connect to the Guest & k3s

Once the VM is running:

1. **Reachability** ‚Äì The guest IP (`vm_ip`) is reachable from the host via the bridge (`br0`). The k3s API is NATed to `https://<host_public_ip>:6443`.
2. **Debug shell (optional)** ‚Äì Debug images allow SSH. Example:
   ```bash
   ssh -p 2222 root@192.168.100.2
   # default password: 123456 (change immediately in debug builds)
   ```
3. **Fetch kubeconfig** ‚Äì Copy `/etc/rancher/k3s/k3s.yaml` from the guest and point it at the bridged endpoint:
   ```bash
   scp -P 2222 root@192.168.100.2:/etc/rancher/k3s/k3s.yaml ~/.kube/chutes.yaml
   sed -i "s/127.0.0.1/<host_public_ip>/" ~/.kube/chutes.yaml
   export KUBECONFIG=~/.kube/chutes.yaml
   kubectl get nodes
   ```
   For non-debug/prod images without SSH, retrieve the kubeconfig via the attested management channel you expose (for example, copy it during image build or use your attestation service to deliver it after the TD quote is verified).
4. **Verify credentials** ‚Äì Check that the miner secret exists:
   ```bash
   kubectl get secret miner-credentials -n chutes -o yaml
   ```
   The bootstrap script `cluster-init/03-k3s-miner-credentials.sh` reads `/var/config/miner-*` and creates both `chutes/miner-credentials` and `attestation-system/miner-credentials`.

---

## 6. Register the TEE Node with Your Control Plane

The sek8s VM already contains the Chutes workloads. To make it part of your miner fleet you only need to introduce it to the **control node** you operate via the `chutes-miner` repo.

1. **Run `chutes-miner` Ansible/Helm on the control node only.** That machine manages your non-TEE workers the usual way while also keeping metadata for TEE nodes.
2. **Install `chutes-miner-cli`** (see the CLI README in the same repo) and authenticate it with your miner account.
3. **Add the TEE VM to your inventory via CLI**, referencing the bridged IP/hostname you configured earlier. Example:
   ```bash
   chutes-miner-cli nodes add \
     --name tee-gpu-0 \
     --type tee \
     --public-ip <host_public_ip> \
     --notes "sek8s Intel TDX VM"
   ```
   Adjust flags to match your environment; the CLI command is authoritative for TEE nodes because there is no SSH session for Ansible to drive.
4. **Reuse the same control node for TEE and non-TEE miners.** Policies, payout settings, and monitoring targets are shared; only the execution environment differs.

> üö´ **Do not add the TEE VM to the `chutes-miner` Ansible inventory.** There is intentionally no SSH access to the sek8s guest, so Ansible cannot run plays against it. Treat it as an appliance managed via the CLI/API instead of configuration management.

You can still use `kubectl` from your workstation to spot-check pods, but day-to-day lifecycle (enrollment, rewards, metadata) should flow through the control node + CLI.

---

## 7. Operate, Monitor, Recycle

- **Lifecycle** ‚Äì Stop everything with `./quick-launch.sh --clean` (unbinds GPUs, tears down bridge, removes PID file). Relaunch with the same config when ready.
- **Logs** ‚Äì Host-side QEMU output lives in `/tmp/tdx-guest-td.log`; Kubernetes events stay inside the guest (`kubectl get events -n chutes`).
- **GPU recovery** ‚Äì If passthrough fails, run `host-tools/scripts/reset-gpus.sh` followed by another launch (or use NVIDIA GPU admin tools per host README).
- **Upgrades** ‚Äì Swap in new guest images by replacing `guest-tools/image/tdx-guest.qcow2` or pointing `TD_IMG` elsewhere, then rerun quick-launch.
- **Security** ‚Äì Protect the config volume‚Äîit holds the plain-text miner seed. Rotate it by rerunning `create-config.sh` with new credentials.

---

## üîó Related References

- [Host preparation guide](../host-tools/README.md)
- [Cache volume details](../host-tools/docs/CACHE.md)
- [Ansible image build overview](../ansible/k3s/README.md)
- [Chutes Miner Helm chart + values](https://github.com/chutesai/chutes-miner)
- [Intel TDX enabling guide](https://cc-enabling.trustedservices.intel.com/intel-tdx-enabling-guide/02/infrastructure_setup/#platform-registration)

Need something else? File an issue or ping the #tdx channel so we can extend this doc.
